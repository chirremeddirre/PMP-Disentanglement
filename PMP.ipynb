{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "830f3976",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85bd189c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c70e1d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms images to a PyTorch Tensor\n",
    "tensor_transform = transforms.ToTensor()\n",
    "  \n",
    "# Download the MNIST Dataset\n",
    "MNIST_dataset = datasets.MNIST(root = \"./data\",\n",
    "                         train = True,\n",
    "                         download = True,\n",
    "                         transform = tensor_transform)\n",
    "  \n",
    "# DataLoader is used to load the dataset \n",
    "# for training\n",
    "MNIST_loader = torch.utils.data.DataLoader(dataset = MNIST_dataset,\n",
    "                                     batch_size = 32,\n",
    "                                     shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a47b72ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "707e191e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CifarB1Dataset(Dataset):\n",
    "    def __init__(self, dic):\n",
    "        self.data = dic[b'data']\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data = torch.tensor(self.data[idx])\n",
    "        data = torch.reshape(data, (3,32,32))/255\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7468f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_batch(batch):\n",
    "    split_size = batch.size(dim=0)//2\n",
    "    x1, x2 = torch.split(batch, split_size)\n",
    "    return x1, x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2273e69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = unpickle(\"cifar-10-batches-py/data_batch_1\")\n",
    "training_data = CifarB1Dataset(dic)\n",
    "batch_size = 50\n",
    "cifar_loader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "train_dataloader = cifar_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad909d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, size = 32, chan = 3):\n",
    "        super().__init__()\n",
    "        self.conv =  nn.Sequential(nn.Conv2d(chan,8,3,stride=1,padding=1),\n",
    "                                     torch.nn.ReLU(),\n",
    "                                     nn.Conv2d(8,16,4,stride=2,padding=1),\n",
    "                                     torch.nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.encoder = nn.Sequential(nn.Linear((size//2)*(size//2)*16, 256),\n",
    "                                     torch.nn.ReLU(),\n",
    "                                     nn.Linear(256,128),\n",
    "                                     torch.nn.ReLU(),\n",
    "                                     nn.Linear(128,64),\n",
    "                                     torch.nn.ReLU(),\n",
    "                                     nn.Linear(64,36),\n",
    "                                     torch.nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.upconv = nn.Sequential(nn.ConvTranspose2d(1,3,3,stride = 2, padding = 1),\n",
    "                                    torch.nn.ReLU(),\n",
    "                                    nn.ConvTranspose2d(3,8,3,stride = 1, padding = 1),\n",
    "                                    torch.nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(nn.Linear(8*11*11,1024),\n",
    "                                     torch.nn.ReLU(),\n",
    "                                     nn.Linear(1024,1536),\n",
    "                                     torch.nn.ReLU(),\n",
    "                                     #nn.Linear(64,128),\n",
    "                                     #torch.nn.ReLU(),\n",
    "                                     #nn.Linear(128,256),\n",
    "                                     #torch.nn.ReLU(),\n",
    "                                     nn.Linear(1536,size*size*chan),\n",
    "                                     torch.nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = 0\n",
    "        if len(x.size()) > 3:\n",
    "            batch_size = x.size()[0]\n",
    "        x = self.conv(x)\n",
    "        x = torch.flatten(x,1)\n",
    "        x = self.encoder(x)\n",
    "        x = torch.reshape(x, (batch_size,1,6,6))\n",
    "        x = self.upconv(x)\n",
    "        #rint(x.size())\n",
    "        x = torch.flatten(x,1)\n",
    "        x = self.decoder(x)\n",
    "        #x = torch.reshape(x, (batch_size, 3, 32, 32))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cde46ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PMP_Encoder(nn.Module):\n",
    "    def  __init__(self,d, chan):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(chan,32,4,stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32,32,4,stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32,64,4,stride=2, padding=1)\n",
    "        self.conv4 = nn.Conv2d(64,64,4,stride=2, padding=1)\n",
    "        self.fc = nn.Linear(64*2*2,256)\n",
    "        self.z = nn.Linear(256, d)\n",
    "    \n",
    "    def forward(self, x1 , x2):\n",
    "        x1 = F.relu(self.conv1(x1))\n",
    "        x1 = F.relu(self.conv2(x1))\n",
    "        x1 = F.relu(self.conv3(x1))\n",
    "        z1 = F.relu(self.conv4(x1))\n",
    "        #print(z1.size())\n",
    "        x2 = F.relu(self.conv1(x2))\n",
    "        x2 = F.relu(self.conv2(x2))\n",
    "        x2 = F.relu(self.conv3(x2))\n",
    "        z2 = F.relu(self.conv4(x2))\n",
    "        z1 = torch.flatten(z1,1)\n",
    "        z1 = F.relu(self.fc(z1))\n",
    "        s1 = self.z(z1)\n",
    "        z2 = torch.flatten(z2,1)\n",
    "        z2 = F.relu(self.fc(z2))\n",
    "        s2 = self.z(z2)\n",
    "        return s1, s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "69dcdccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PMP_Decoder(nn.Module):\n",
    "    def  __init__(self, d, chan):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(d,256)\n",
    "        self.fc2 = nn.Linear(256,256)\n",
    "        self.fc3 = nn.Linear(256, 64*2*2)\n",
    "        # Reshape into 64x4x4\n",
    "        self.convt1 = nn.ConvTranspose2d(64, 64, 4,stride=2,padding=1)\n",
    "        self.convt2 = nn.ConvTranspose2d(64, 32, 4,stride=2,padding=1)\n",
    "        self.convt3 = nn.ConvTranspose2d(32, 32, 4,stride=2,padding=1)\n",
    "        self.convt4 = nn.ConvTranspose2d(32, chan, 4,stride=2,padding=1)\n",
    "    \n",
    "    def forward(self, z1, z2, batch_size):\n",
    "        z1 = F.relu(self.fc1(z1))\n",
    "        z1 = F.relu(self.fc2(z1))\n",
    "        z1 = F.relu(self.fc3(z1))\n",
    "        z2 = F.relu(self.fc1(z2))\n",
    "        z2 = F.relu(self.fc2(z2))\n",
    "        z2 = F.relu(self.fc3(z2))\n",
    "        #print(z1.size())\n",
    "        #print(f\"batch_size = {batch_size}\")\n",
    "        z1 = torch.reshape(z1, (batch_size, 64,2,2))\n",
    "        z2 = torch.reshape(z2, (batch_size, 64,2,2))\n",
    "\n",
    "        z1 = F.relu(self.convt1(z1))\n",
    "        z1 = F.relu(self.convt2(z1))\n",
    "        z1 = F.relu(self.convt3(z1))\n",
    "        x1 = F.relu(self.convt4(z1))\n",
    "\n",
    "        z2 = F.relu(self.convt1(z2))\n",
    "        z2 = F.relu(self.convt2(z2))\n",
    "        z2 = F.relu(self.convt3(z2))\n",
    "        x2 = F.relu(self.convt4(z2))\n",
    "\n",
    "        return x1, x2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9579f7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PMP(nn.Module):\n",
    "    def __init__(self, d=10, chan=3, batch_size=2):\n",
    "        super(PMP, self).__init__()\n",
    "        self.d = d\n",
    "        self.chan = chan\n",
    "        self.batch_size = batch_size\n",
    "        self.encoder = PMP_Encoder(self.d, self.chan)\n",
    "        self.decoder = PMP_Decoder(self.d, self.chan)\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        z1, z2 = self.encoder(x1, x2)\n",
    "        x1_, x2_ = self.decoder(z1, z2, self.batch_size)\n",
    "        return x1_, x2_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "adfa6ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PMP(\n",
      "  (encoder): PMP_Encoder(\n",
      "    (conv1): Conv2d(3, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (conv2): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (conv3): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (conv4): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (fc): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (z): Linear(in_features=256, out_features=10, bias=True)\n",
      "  )\n",
      "  (decoder): PMP_Decoder(\n",
      "    (fc1): Linear(in_features=10, out_features=256, bias=True)\n",
      "    (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (fc3): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (convt1): ConvTranspose2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (convt2): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (convt3): ConvTranspose2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (convt4): ConvTranspose2d(32, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "size = 32\n",
    "channels = 3\n",
    "net = PMP(batch_size=batch_size//2)\n",
    "print(net)\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(net.parameters(),\n",
    "                             lr = 0.00003,\n",
    "                             weight_decay = 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "731a95d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     1] loss: 0.000202\n",
      "[1,    21] loss: 0.004270\n",
      "[1,    41] loss: 0.004149\n",
      "[1,    61] loss: 0.004155\n",
      "[1,    81] loss: 0.004204\n",
      "[1,   101] loss: 0.004178\n",
      "[1,   121] loss: 0.004080\n",
      "[1,   141] loss: 0.003672\n",
      "[1,   161] loss: 0.003114\n",
      "[1,   181] loss: 0.002701\n",
      "[2,     1] loss: 0.000123\n",
      "[2,    21] loss: 0.002386\n",
      "[2,    41] loss: 0.002261\n",
      "[2,    61] loss: 0.002195\n",
      "[2,    81] loss: 0.002129\n",
      "[2,   101] loss: 0.002109\n",
      "[2,   121] loss: 0.002094\n",
      "[2,   141] loss: 0.002023\n",
      "[2,   161] loss: 0.002020\n",
      "[2,   181] loss: 0.002006\n",
      "[3,     1] loss: 0.000099\n",
      "[3,    21] loss: 0.001984\n",
      "[3,    41] loss: 0.001943\n",
      "[3,    61] loss: 0.001938\n",
      "[3,    81] loss: 0.001912\n",
      "[3,   101] loss: 0.001933\n",
      "[3,   121] loss: 0.001873\n",
      "[3,   141] loss: 0.001869\n",
      "[3,   161] loss: 0.001881\n",
      "[3,   181] loss: 0.001877\n",
      "[4,     1] loss: 0.000082\n",
      "[4,    21] loss: 0.001816\n",
      "[4,    41] loss: 0.001827\n",
      "[4,    61] loss: 0.001825\n",
      "[4,    81] loss: 0.001800\n",
      "[4,   101] loss: 0.001811\n",
      "[4,   121] loss: 0.001731\n",
      "[4,   141] loss: 0.001781\n",
      "[4,   161] loss: 0.001725\n",
      "[4,   181] loss: 0.001703\n",
      "[5,     1] loss: 0.000075\n",
      "[5,    21] loss: 0.001681\n",
      "[5,    41] loss: 0.001654\n",
      "[5,    61] loss: 0.001659\n",
      "[5,    81] loss: 0.001620\n",
      "[5,   101] loss: 0.001615\n",
      "[5,   121] loss: 0.001630\n",
      "[5,   141] loss: 0.001630\n",
      "[5,   161] loss: 0.001579\n",
      "[5,   181] loss: 0.001608\n",
      "[6,     1] loss: 0.000081\n",
      "[6,    21] loss: 0.001635\n",
      "[6,    41] loss: 0.001597\n",
      "[6,    61] loss: 0.001565\n",
      "[6,    81] loss: 0.001600\n",
      "[6,   101] loss: 0.001586\n",
      "[6,   121] loss: 0.001592\n",
      "[6,   141] loss: 0.001600\n",
      "[6,   161] loss: 0.001583\n",
      "[6,   181] loss: 0.001618\n",
      "[7,     1] loss: 0.000081\n",
      "[7,    21] loss: 0.001608\n",
      "[7,    41] loss: 0.001575\n",
      "[7,    61] loss: 0.001566\n",
      "[7,    81] loss: 0.001585\n",
      "[7,   101] loss: 0.001586\n",
      "[7,   121] loss: 0.001576\n",
      "[7,   141] loss: 0.001573\n",
      "[7,   161] loss: 0.001560\n",
      "[7,   181] loss: 0.001566\n",
      "[8,     1] loss: 0.000074\n",
      "[8,    21] loss: 0.001567\n",
      "[8,    41] loss: 0.001570\n",
      "[8,    61] loss: 0.001540\n",
      "[8,    81] loss: 0.001580\n",
      "[8,   101] loss: 0.001497\n",
      "[8,   121] loss: 0.001523\n",
      "[8,   141] loss: 0.001531\n",
      "[8,   161] loss: 0.001496\n",
      "[8,   181] loss: 0.001464\n",
      "[9,     1] loss: 0.000070\n",
      "[9,    21] loss: 0.001494\n",
      "[9,    41] loss: 0.001475\n",
      "[9,    61] loss: 0.001476\n",
      "[9,    81] loss: 0.001435\n",
      "[9,   101] loss: 0.001487\n",
      "[9,   121] loss: 0.001467\n",
      "[9,   141] loss: 0.001446\n",
      "[9,   161] loss: 0.001451\n",
      "[9,   181] loss: 0.001452\n",
      "[10,     1] loss: 0.000066\n",
      "[10,    21] loss: 0.001426\n",
      "[10,    41] loss: 0.001415\n",
      "[10,    61] loss: 0.001426\n",
      "[10,    81] loss: 0.001408\n",
      "[10,   101] loss: 0.001429\n",
      "[10,   121] loss: 0.001447\n",
      "[10,   141] loss: 0.001405\n",
      "[10,   161] loss: 0.001391\n",
      "[10,   181] loss: 0.001408\n",
      "[11,     1] loss: 0.000064\n",
      "[11,    21] loss: 0.001404\n",
      "[11,    41] loss: 0.001392\n",
      "[11,    61] loss: 0.001454\n",
      "[11,    81] loss: 0.001417\n",
      "[11,   101] loss: 0.001414\n",
      "[11,   121] loss: 0.001390\n",
      "[11,   141] loss: 0.001407\n",
      "[11,   161] loss: 0.001429\n",
      "[11,   181] loss: 0.001383\n",
      "[12,     1] loss: 0.000066\n",
      "[12,    21] loss: 0.001387\n",
      "[12,    41] loss: 0.001382\n",
      "[12,    61] loss: 0.001415\n",
      "[12,    81] loss: 0.001380\n",
      "[12,   101] loss: 0.001380\n",
      "[12,   121] loss: 0.001391\n",
      "[12,   141] loss: 0.001400\n",
      "[12,   161] loss: 0.001421\n",
      "[12,   181] loss: 0.001421\n",
      "[13,     1] loss: 0.000072\n",
      "[13,    21] loss: 0.001400\n",
      "[13,    41] loss: 0.001395\n",
      "[13,    61] loss: 0.001391\n",
      "[13,    81] loss: 0.001399\n",
      "[13,   101] loss: 0.001390\n",
      "[13,   121] loss: 0.001365\n",
      "[13,   141] loss: 0.001386\n",
      "[13,   161] loss: 0.001382\n",
      "[13,   181] loss: 0.001391\n",
      "[14,     1] loss: 0.000074\n",
      "[14,    21] loss: 0.001411\n",
      "[14,    41] loss: 0.001368\n",
      "[14,    61] loss: 0.001394\n",
      "[14,    81] loss: 0.001370\n",
      "[14,   101] loss: 0.001383\n",
      "[14,   121] loss: 0.001395\n",
      "[14,   141] loss: 0.001367\n",
      "[14,   161] loss: 0.001383\n",
      "[14,   181] loss: 0.001378\n",
      "[15,     1] loss: 0.000069\n",
      "[15,    21] loss: 0.001407\n",
      "[15,    41] loss: 0.001372\n",
      "[15,    61] loss: 0.001376\n",
      "[15,    81] loss: 0.001346\n",
      "[15,   101] loss: 0.001359\n",
      "[15,   121] loss: 0.001380\n",
      "[15,   141] loss: 0.001368\n",
      "[15,   161] loss: 0.001388\n",
      "[15,   181] loss: 0.001376\n",
      "[16,     1] loss: 0.000070\n",
      "[16,    21] loss: 0.001344\n",
      "[16,    41] loss: 0.001389\n",
      "[16,    61] loss: 0.001375\n",
      "[16,    81] loss: 0.001374\n",
      "[16,   101] loss: 0.001341\n",
      "[16,   121] loss: 0.001359\n",
      "[16,   141] loss: 0.001381\n",
      "[16,   161] loss: 0.001368\n",
      "[16,   181] loss: 0.001364\n",
      "[17,     1] loss: 0.000069\n",
      "[17,    21] loss: 0.001341\n",
      "[17,    41] loss: 0.001348\n",
      "[17,    61] loss: 0.001363\n",
      "[17,    81] loss: 0.001373\n",
      "[17,   101] loss: 0.001398\n",
      "[17,   121] loss: 0.001396\n",
      "[17,   141] loss: 0.001357\n",
      "[17,   161] loss: 0.001358\n",
      "[17,   181] loss: 0.001368\n",
      "[18,     1] loss: 0.000071\n",
      "[18,    21] loss: 0.001391\n",
      "[18,    41] loss: 0.001343\n",
      "[18,    61] loss: 0.001366\n",
      "[18,    81] loss: 0.001333\n",
      "[18,   101] loss: 0.001360\n",
      "[18,   121] loss: 0.001378\n",
      "[18,   141] loss: 0.001361\n",
      "[18,   161] loss: 0.001361\n",
      "[18,   181] loss: 0.001358\n",
      "[19,     1] loss: 0.000067\n",
      "[19,    21] loss: 0.001358\n",
      "[19,    41] loss: 0.001364\n",
      "[19,    61] loss: 0.001372\n",
      "[19,    81] loss: 0.001366\n",
      "[19,   101] loss: 0.001378\n",
      "[19,   121] loss: 0.001370\n",
      "[19,   141] loss: 0.001363\n",
      "[19,   161] loss: 0.001367\n",
      "[19,   181] loss: 0.001367\n",
      "[20,     1] loss: 0.000068\n",
      "[20,    21] loss: 0.001369\n",
      "[20,    41] loss: 0.001324\n",
      "[20,    61] loss: 0.001353\n",
      "[20,    81] loss: 0.001359\n",
      "[20,   101] loss: 0.001393\n",
      "[20,   121] loss: 0.001362\n",
      "[20,   141] loss: 0.001355\n",
      "[20,   161] loss: 0.001396\n",
      "[20,   181] loss: 0.001354\n",
      "[21,     1] loss: 0.000064\n",
      "[21,    21] loss: 0.001379\n",
      "[21,    41] loss: 0.001378\n",
      "[21,    61] loss: 0.001370\n",
      "[21,    81] loss: 0.001365\n",
      "[21,   101] loss: 0.001371\n",
      "[21,   121] loss: 0.001354\n",
      "[21,   141] loss: 0.001378\n",
      "[21,   161] loss: 0.001329\n",
      "[21,   181] loss: 0.001376\n",
      "[22,     1] loss: 0.000068\n",
      "[22,    21] loss: 0.001377\n",
      "[22,    41] loss: 0.001371\n",
      "[22,    61] loss: 0.001359\n",
      "[22,    81] loss: 0.001376\n",
      "[22,   101] loss: 0.001367\n",
      "[22,   121] loss: 0.001353\n",
      "[22,   141] loss: 0.001346\n",
      "[22,   161] loss: 0.001342\n",
      "[22,   181] loss: 0.001366\n",
      "[23,     1] loss: 0.000069\n",
      "[23,    21] loss: 0.001388\n",
      "[23,    41] loss: 0.001347\n",
      "[23,    61] loss: 0.001371\n",
      "[23,    81] loss: 0.001376\n",
      "[23,   101] loss: 0.001358\n",
      "[23,   121] loss: 0.001342\n",
      "[23,   141] loss: 0.001355\n",
      "[23,   161] loss: 0.001338\n",
      "[23,   181] loss: 0.001349\n",
      "[24,     1] loss: 0.000063\n",
      "[24,    21] loss: 0.001382\n",
      "[24,    41] loss: 0.001355\n",
      "[24,    61] loss: 0.001327\n",
      "[24,    81] loss: 0.001344\n",
      "[24,   101] loss: 0.001342\n",
      "[24,   121] loss: 0.001377\n",
      "[24,   141] loss: 0.001351\n",
      "[24,   161] loss: 0.001325\n",
      "[24,   181] loss: 0.001363\n",
      "[25,     1] loss: 0.000073\n",
      "[25,    21] loss: 0.001353\n",
      "[25,    41] loss: 0.001357\n",
      "[25,    61] loss: 0.001332\n",
      "[25,    81] loss: 0.001344\n",
      "[25,   101] loss: 0.001349\n",
      "[25,   121] loss: 0.001356\n",
      "[25,   141] loss: 0.001322\n",
      "[25,   161] loss: 0.001369\n",
      "[25,   181] loss: 0.001337\n",
      "[26,     1] loss: 0.000062\n",
      "[26,    21] loss: 0.001354\n",
      "[26,    41] loss: 0.001375\n",
      "[26,    61] loss: 0.001340\n",
      "[26,    81] loss: 0.001346\n",
      "[26,   101] loss: 0.001331\n",
      "[26,   121] loss: 0.001348\n",
      "[26,   141] loss: 0.001345\n",
      "[26,   161] loss: 0.001297\n",
      "[26,   181] loss: 0.001303\n",
      "[27,     1] loss: 0.000064\n",
      "[27,    21] loss: 0.001311\n",
      "[27,    41] loss: 0.001307\n",
      "[27,    61] loss: 0.001334\n",
      "[27,    81] loss: 0.001326\n",
      "[27,   101] loss: 0.001347\n",
      "[27,   121] loss: 0.001347\n",
      "[27,   141] loss: 0.001341\n",
      "[27,   161] loss: 0.001330\n",
      "[27,   181] loss: 0.001334\n",
      "[28,     1] loss: 0.000061\n",
      "[28,    21] loss: 0.001344\n",
      "[28,    41] loss: 0.001286\n",
      "[28,    61] loss: 0.001339\n",
      "[28,    81] loss: 0.001306\n",
      "[28,   101] loss: 0.001338\n",
      "[28,   121] loss: 0.001313\n",
      "[28,   141] loss: 0.001296\n",
      "[28,   161] loss: 0.001310\n",
      "[28,   181] loss: 0.001318\n",
      "[29,     1] loss: 0.000070\n",
      "[29,    21] loss: 0.001300\n",
      "[29,    41] loss: 0.001309\n",
      "[29,    61] loss: 0.001298\n",
      "[29,    81] loss: 0.001317\n",
      "[29,   101] loss: 0.001334\n",
      "[29,   121] loss: 0.001294\n",
      "[29,   141] loss: 0.001318\n",
      "[29,   161] loss: 0.001287\n",
      "[29,   181] loss: 0.001297\n",
      "[30,     1] loss: 0.000065\n",
      "[30,    21] loss: 0.001285\n",
      "[30,    41] loss: 0.001303\n",
      "[30,    61] loss: 0.001342\n",
      "[30,    81] loss: 0.001347\n",
      "[30,   101] loss: 0.001267\n",
      "[30,   121] loss: 0.001307\n",
      "[30,   141] loss: 0.001301\n",
      "[30,   161] loss: 0.001325\n",
      "[30,   181] loss: 0.001294\n",
      "[31,     1] loss: 0.000061\n",
      "[31,    21] loss: 0.001290\n",
      "[31,    41] loss: 0.001290\n",
      "[31,    61] loss: 0.001316\n",
      "[31,    81] loss: 0.001293\n",
      "[31,   101] loss: 0.001307\n",
      "[31,   121] loss: 0.001299\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31,   141] loss: 0.001296\n",
      "[31,   161] loss: 0.001301\n",
      "[31,   181] loss: 0.001319\n",
      "[32,     1] loss: 0.000067\n",
      "[32,    21] loss: 0.001279\n",
      "[32,    41] loss: 0.001258\n",
      "[32,    61] loss: 0.001347\n",
      "[32,    81] loss: 0.001299\n",
      "[32,   101] loss: 0.001257\n",
      "[32,   121] loss: 0.001304\n",
      "[32,   141] loss: 0.001282\n",
      "[32,   161] loss: 0.001314\n",
      "[32,   181] loss: 0.001289\n",
      "[33,     1] loss: 0.000060\n",
      "[33,    21] loss: 0.001272\n",
      "[33,    41] loss: 0.001283\n",
      "[33,    61] loss: 0.001297\n",
      "[33,    81] loss: 0.001292\n",
      "[33,   101] loss: 0.001274\n",
      "[33,   121] loss: 0.001290\n",
      "[33,   141] loss: 0.001288\n",
      "[33,   161] loss: 0.001289\n",
      "[33,   181] loss: 0.001274\n",
      "[34,     1] loss: 0.000065\n",
      "[34,    21] loss: 0.001284\n",
      "[34,    41] loss: 0.001304\n",
      "[34,    61] loss: 0.001297\n",
      "[34,    81] loss: 0.001273\n",
      "[34,   101] loss: 0.001272\n",
      "[34,   121] loss: 0.001283\n",
      "[34,   141] loss: 0.001302\n",
      "[34,   161] loss: 0.001264\n",
      "[34,   181] loss: 0.001283\n",
      "[35,     1] loss: 0.000067\n",
      "[35,    21] loss: 0.001268\n",
      "[35,    41] loss: 0.001284\n",
      "[35,    61] loss: 0.001271\n",
      "[35,    81] loss: 0.001283\n",
      "[35,   101] loss: 0.001287\n",
      "[35,   121] loss: 0.001279\n",
      "[35,   141] loss: 0.001288\n",
      "[35,   161] loss: 0.001270\n",
      "[35,   181] loss: 0.001270\n",
      "[36,     1] loss: 0.000056\n",
      "[36,    21] loss: 0.001283\n",
      "[36,    41] loss: 0.001277\n",
      "[36,    61] loss: 0.001286\n",
      "[36,    81] loss: 0.001312\n",
      "[36,   101] loss: 0.001275\n",
      "[36,   121] loss: 0.001265\n",
      "[36,   141] loss: 0.001281\n",
      "[36,   161] loss: 0.001248\n",
      "[36,   181] loss: 0.001283\n",
      "[37,     1] loss: 0.000065\n",
      "[37,    21] loss: 0.001271\n",
      "[37,    41] loss: 0.001298\n",
      "[37,    61] loss: 0.001274\n",
      "[37,    81] loss: 0.001287\n",
      "[37,   101] loss: 0.001278\n",
      "[37,   121] loss: 0.001271\n",
      "[37,   141] loss: 0.001286\n",
      "[37,   161] loss: 0.001278\n",
      "[37,   181] loss: 0.001267\n",
      "[38,     1] loss: 0.000061\n",
      "[38,    21] loss: 0.001280\n",
      "[38,    41] loss: 0.001270\n",
      "[38,    61] loss: 0.001262\n",
      "[38,    81] loss: 0.001277\n",
      "[38,   101] loss: 0.001267\n",
      "[38,   121] loss: 0.001282\n",
      "[38,   141] loss: 0.001267\n",
      "[38,   161] loss: 0.001279\n",
      "[38,   181] loss: 0.001264\n",
      "[39,     1] loss: 0.000064\n",
      "[39,    21] loss: 0.001247\n",
      "[39,    41] loss: 0.001267\n",
      "[39,    61] loss: 0.001294\n",
      "[39,    81] loss: 0.001266\n",
      "[39,   101] loss: 0.001273\n",
      "[39,   121] loss: 0.001267\n",
      "[39,   141] loss: 0.001260\n",
      "[39,   161] loss: 0.001294\n",
      "[39,   181] loss: 0.001270\n",
      "[40,     1] loss: 0.000061\n",
      "[40,    21] loss: 0.001282\n",
      "[40,    41] loss: 0.001255\n",
      "[40,    61] loss: 0.001267\n",
      "[40,    81] loss: 0.001256\n",
      "[40,   101] loss: 0.001282\n",
      "[40,   121] loss: 0.001279\n",
      "[40,   141] loss: 0.001238\n",
      "[40,   161] loss: 0.001225\n",
      "[40,   181] loss: 0.001281\n",
      "[41,     1] loss: 0.000071\n",
      "[41,    21] loss: 0.001256\n",
      "[41,    41] loss: 0.001259\n",
      "[41,    61] loss: 0.001250\n",
      "[41,    81] loss: 0.001246\n",
      "[41,   101] loss: 0.001250\n",
      "[41,   121] loss: 0.001253\n",
      "[41,   141] loss: 0.001218\n",
      "[41,   161] loss: 0.001269\n",
      "[41,   181] loss: 0.001265\n",
      "[42,     1] loss: 0.000061\n",
      "[42,    21] loss: 0.001236\n",
      "[42,    41] loss: 0.001243\n",
      "[42,    61] loss: 0.001229\n",
      "[42,    81] loss: 0.001254\n",
      "[42,   101] loss: 0.001249\n",
      "[42,   121] loss: 0.001231\n",
      "[42,   141] loss: 0.001240\n",
      "[42,   161] loss: 0.001243\n",
      "[42,   181] loss: 0.001232\n",
      "[43,     1] loss: 0.000068\n",
      "[43,    21] loss: 0.001244\n",
      "[43,    41] loss: 0.001224\n",
      "[43,    61] loss: 0.001253\n",
      "[43,    81] loss: 0.001245\n",
      "[43,   101] loss: 0.001217\n",
      "[43,   121] loss: 0.001224\n",
      "[43,   141] loss: 0.001256\n",
      "[43,   161] loss: 0.001240\n",
      "[43,   181] loss: 0.001256\n",
      "[44,     1] loss: 0.000057\n",
      "[44,    21] loss: 0.001258\n",
      "[44,    41] loss: 0.001258\n",
      "[44,    61] loss: 0.001220\n",
      "[44,    81] loss: 0.001241\n",
      "[44,   101] loss: 0.001230\n",
      "[44,   121] loss: 0.001237\n",
      "[44,   141] loss: 0.001248\n",
      "[44,   161] loss: 0.001239\n",
      "[44,   181] loss: 0.001252\n",
      "[45,     1] loss: 0.000060\n",
      "[45,    21] loss: 0.001254\n",
      "[45,    41] loss: 0.001232\n",
      "[45,    61] loss: 0.001234\n",
      "[45,    81] loss: 0.001240\n",
      "[45,   101] loss: 0.001244\n",
      "[45,   121] loss: 0.001245\n",
      "[45,   141] loss: 0.001209\n",
      "[45,   161] loss: 0.001264\n",
      "[45,   181] loss: 0.001275\n",
      "[46,     1] loss: 0.000060\n",
      "[46,    21] loss: 0.001258\n",
      "[46,    41] loss: 0.001234\n",
      "[46,    61] loss: 0.001277\n",
      "[46,    81] loss: 0.001240\n",
      "[46,   101] loss: 0.001228\n",
      "[46,   121] loss: 0.001256\n",
      "[46,   141] loss: 0.001246\n",
      "[46,   161] loss: 0.001235\n",
      "[46,   181] loss: 0.001231\n",
      "[47,     1] loss: 0.000060\n",
      "[47,    21] loss: 0.001229\n",
      "[47,    41] loss: 0.001231\n",
      "[47,    61] loss: 0.001248\n",
      "[47,    81] loss: 0.001236\n",
      "[47,   101] loss: 0.001254\n",
      "[47,   121] loss: 0.001247\n",
      "[47,   141] loss: 0.001230\n",
      "[47,   161] loss: 0.001238\n",
      "[47,   181] loss: 0.001205\n",
      "[48,     1] loss: 0.000061\n",
      "[48,    21] loss: 0.001237\n",
      "[48,    41] loss: 0.001249\n",
      "[48,    61] loss: 0.001235\n",
      "[48,    81] loss: 0.001263\n",
      "[48,   101] loss: 0.001218\n",
      "[48,   121] loss: 0.001241\n",
      "[48,   141] loss: 0.001237\n",
      "[48,   161] loss: 0.001247\n",
      "[48,   181] loss: 0.001229\n",
      "[49,     1] loss: 0.000067\n",
      "[49,    21] loss: 0.001237\n",
      "[49,    41] loss: 0.001243\n",
      "[49,    61] loss: 0.001246\n",
      "[49,    81] loss: 0.001218\n",
      "[49,   101] loss: 0.001216\n",
      "[49,   121] loss: 0.001217\n",
      "[49,   141] loss: 0.001244\n",
      "[49,   161] loss: 0.001229\n",
      "[49,   181] loss: 0.001257\n",
      "[50,     1] loss: 0.000063\n",
      "[50,    21] loss: 0.001214\n",
      "[50,    41] loss: 0.001254\n",
      "[50,    61] loss: 0.001240\n",
      "[50,    81] loss: 0.001218\n",
      "[50,   101] loss: 0.001282\n",
      "[50,   121] loss: 0.001226\n",
      "[50,   141] loss: 0.001257\n",
      "[50,   161] loss: 0.001233\n",
      "[50,   181] loss: 0.001247\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(50):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_dataloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        x1, x2 = split_batch(data.float())\n",
    "        #print(x1.type())\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        x1_hat, x2_hat = net(x1,x2)\n",
    "        loss = criterion(x1, x1_hat)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 20 == 0:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.6f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "83a0de40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "im = torch.reshape(x1_hat[5],(channels,size,size))\n",
    "im = im.detach().numpy()\n",
    "im = np.moveaxis(im,0,2)\n",
    "im_l = torch.reshape(x1[5],(channels,size,size)).detach().numpy()\n",
    "im_l = np.moveaxis(im_l,0,2)\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.add_subplot(1,2,1)\n",
    "plt.imshow(im_l)\n",
    "fig.add_subplot(1,2,2)\n",
    "plt.imshow(im)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a58bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "im = torch.reshape(x1_hat[0],(3,32,32)) #torch.movedim(x1_hat,0,2)\n",
    "im = im.detach().numpy()\n",
    "#print(im.shape)\n",
    "#print(x1_hat.size())\n",
    "plt.imshow(im.T)\n",
    "im_l = torch.reshape(x1[0],(3,32,32)).detach().numpy()\n",
    "plt.imshow(im_l.T)\n",
    "#X = dic[b'data']\n",
    "#X = X.reshape(10000, 3, 32, 32).transpose(0,2,3,1).astype(\"uint8\")\n",
    "#im1 =np.array(dic[b'data'][5]).reshape(3,32,32).T\n",
    "\n",
    "print(im)\n",
    "#plt.imshow(im1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e8c1c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
